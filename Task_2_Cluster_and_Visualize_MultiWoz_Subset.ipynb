{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55e7b44b",
   "metadata": {},
   "source": [
    "### JSALT'23 Computer Lab Part II\n",
    "\n",
    "- In this part, we will use simple strategies to analyze and visualize human-to-human task-oriented conversations.\n",
    "\n",
    "- For this, we will use a standard dataset called MultiWoz v2.2, that covers several task-oriented domains such as\n",
    "restaurant, taxi, hotel, train bookings, tourist attractions, and even combination of these domains.\n",
    "\n",
    "- However, we will use only a subset of the conversations and try to find a strucutre within the conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2d7f7e",
   "metadata": {},
   "source": [
    "### Hardware\n",
    "- We can use either Google collab or Kaggle notebooks or your personal laptops (with a GPU)\n",
    "- Some part of the assignment requires a GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324455a1",
   "metadata": {},
   "source": [
    "### Install required packages in a Kaggle notebook\n",
    "```\n",
    "! pip3 install sentence-transformers==2.2.2\n",
    "! pip3 install datasets==2.12\n",
    "! pip3 install scikit-learn\n",
    "! pip3 install graphviz\n",
    "! pip3 install matplotlib \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7e945b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install sentence-transformers==2.2.2\n",
    "! pip3 install datasets==2.12\n",
    "! pip3 install scikit-learn\n",
    "! pip3 install graphviz\n",
    "! pip3 install matplotlib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b0ad64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from copy import deepcopy\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c091ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524ff0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans, KMeans\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "import graphviz  \n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4578887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_multiwoz_subset(domain):\n",
    "    \"\"\"Pick a subset of dialogues with given domain only.\n",
    "    Since each dialogue can contain multiple domains, the selected\n",
    "    subset will still be multi-domain with the given domain being dominant\"\"\"\n",
    "\n",
    "    multiwoz_dset = load_dataset(\"multi_woz_v22\")\n",
    "\n",
    "    subset_ixs = []  # list\n",
    "    subset_utts = []  # nested list\n",
    "    \n",
    "    services_selected = []\n",
    "    \n",
    "    for i, conv in enumerate(multiwoz_dset[\"train\"]):\n",
    "        # keys: dialogue_id, services, turns\n",
    "        if domain in conv[\"services\"]:\n",
    "            utts = conv[\"turns\"][\"utterance\"]\n",
    "            subset_utts.append(utts)\n",
    "            subset_ixs.append(i)\n",
    "            services_selected.append(\"_\".join(sorted(conv['services'])))\n",
    "    \n",
    "    return subset_utts, subset_ixs, services_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab75cddf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# In Multiwoz dataset, conversations with a single domain rarely exist.\n",
    "# Most of the convsersations involve multiple domains, hence \"multi\"-woz\n",
    "\n",
    "domain = 'restaurant'\n",
    "\n",
    "domain_convs, domain_ixs, services_selected = load_multiwoz_subset(domain)\n",
    "print('Conversations with domain:', domain, '=', len(domain_convs))\n",
    "print(\"Services selected\\n \", \"\\n  - \".join(list(set(services_selected))))\n",
    "\n",
    "utts_per_conv = np.asarray([len(conv) for conv in domain_convs])\n",
    "print('\\nTotal number of utterances (sentences):', np.sum(utts_per_conv))\n",
    "avg_utts_per_conv = np.mean(utts_per_conv)\n",
    "print('Average number of utts per conversation: {:.1f}'.format(avg_utts_per_conv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bba1869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentence_embeddings(sentences, model_name, out_emb_file, max_s=512):\n",
    "    \"\"\"Extract sentence embeddings using one of the models from Sentence_transformers\n",
    "    See https://huggingface.co/sentence-transformers for list of supported models.\n",
    "    \n",
    "    Args:\n",
    "    -----\n",
    "        sentences (list): List of sentences where every sentence in a plain string\n",
    "        model_name (str): Should match the name from sentence_transformers (eg: sentence-transformers/LaBSE)\n",
    "        out_emb_file (str): File where the embeddings are saved, so that we can \n",
    "            load them from the disk\n",
    "        max_s (int): Maximum sequence length, usually 512, but can be higher/lower of different models.\n",
    "    \"\"\"    \n",
    "\n",
    "    if os.path.exists(out_emb_file):\n",
    "        embs = np.load(emb_file)\n",
    "        print('Re-loading sentence embeddings from file:', out_emb_file,)\n",
    "        print('  Embeddings shape:', embs.shape)\n",
    "        \n",
    "        if embs.shape[0] != len(sentences):\n",
    "            print(\"Loaded embeddings {:d} != number of sentences {:d}.\".format(embs.shape[0], len(sentences)),\n",
    "                  \"Probably the embeddings loaded from the disk correspond to different set of sentences.\")\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        print(\"Extracting embeddings..\")\n",
    "        \n",
    "        model = SentenceTransformer(model_name)\n",
    "        \n",
    "        # this should not exceed the max seq length that was used\n",
    "        # while training the model\n",
    "        model.max_sequence_length = max_s  \n",
    "        # Consider using a GPU accelarator for this step\n",
    "        # and uncomment the following line\n",
    "        model.cuda()\n",
    "        \n",
    "        embs = model.encode(sentences)\n",
    "        \n",
    "        np.save(emb_file, embs)\n",
    "        print(\"Saved embeddings\", embs.shape, \"to\", emb_file)\n",
    "        \n",
    "    return embs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad787dd3",
   "metadata": {},
   "source": [
    "### Strategies to represent the dialogues within in conversation\n",
    "\n",
    "1. Treat each utterance independent of others (naive)\n",
    "2. Consider the `n` past history (or context) to represent current utterance.\n",
    "3. Consider the `n` past and `n` future utterances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d60fefc",
   "metadata": {},
   "source": [
    "#### Strategy 1: naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68629e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will flatten the conversations into independent sentences, but keeping the order\n",
    "# We can try better strategies to obtain sentence embeddings, \n",
    "# eg: concatenating 1 or 2 previous sentences to the current one, etc.\n",
    "\n",
    "# strategy 1: treat every sentence independent of each other\n",
    "sentences_indep = []\n",
    "for sents in domain_convs:\n",
    "    sentences_indep.extend(sents)\n",
    "print(\"Flattened sentences (independent):\", len(sentences_indep))\n",
    "\n",
    "model_name = \"sentence-transformers/LaBSE\"\n",
    "emb_file = f\"cluster_multiwoz/{domain}_sent_embs.npy\"\n",
    "\n",
    "embs_indep = extract_sentence_embeddings(sentences_indep, model_name, emb_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381b6111",
   "metadata": {},
   "source": [
    "#### Strategy 2: Combine past utterance with the current one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fe5df3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Running this cell will override the embeddings from previous cell\n",
    "# appending previouse sentence to the current one and extracting embeddings\n",
    "# sent_0, sent_0+sent_1, \n",
    "# Strategy 2: Prepend the current utterance with past utterance\n",
    "\n",
    "sentences_hist1 = []\n",
    "for sents in domain_convs:\n",
    "    history = \"\"\n",
    "    for i, sent in enumerate(sents):\n",
    "        sentences_hist1.append(history + sent)        \n",
    "        history = sent + \" \"\n",
    "print(\"Sentences prepended with past 1 history:\", len(sentences_hist1))\n",
    "\n",
    "model_name = \"sentence-transformers/LaBSE\"\n",
    "emb_file = f\"cluster_multiwoz/{domain}_accum_h1_sent_embs.npy\"\n",
    "\n",
    "embs_hist1 = extract_sentence_embeddings(sentences_hist1, model_name, emb_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d5ed67",
   "metadata": {},
   "source": [
    "### Selecting a smaller subset from the `domain=restaurant`\n",
    "- Skip this step for the first time.\n",
    "- Try to visualize the clusters with all the conversations, later, pick only a subset and visualize again.\n",
    "- In the following example, we will consider even a smaller subset, i.e., by selecting services that \n",
    "based on `restaurant` and `restaurant + taxi` booking.\n",
    "- In reality, we can assume that we will have such kind of metadata available for conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fc30a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only embeddings that belong to few services\n",
    "#\n",
    "def select_subset_serivces(subset_services, embeddings):\n",
    "\n",
    "    subset_sents = []\n",
    "    subset_embs = []\n",
    "    subset_domain_convs = []\n",
    "\n",
    "    j = 0\n",
    "    for i, serv in enumerate(services_selected):\n",
    "        # print(i, serv, len(domain_convs[i]), \"::\", j, \"->\", j+len(domain_convs[i]))\n",
    "        if serv in subset_services:\n",
    "            # print(i, serv, \"::\", j, \"->\", j+len(domain_convs[i]), '::', embs[j:j+len(domain_convs[i])].shape)\n",
    "\n",
    "            subset_domain_convs.append(domain_convs[i])\n",
    "            subset_embs.append(embeddings[j:j+len(domain_convs[i]),])\n",
    "\n",
    "            subset_sents.extend(domain_convs[i])\n",
    "\n",
    "        j += len(domain_convs[i])\n",
    "        \n",
    "    return subset_domain_convs, subset_sents, subset_embs\n",
    "\n",
    "\n",
    "\n",
    "subset_services = (\"restaurant\", \"restaurant_taxi\")\n",
    "\n",
    "# we are can decide which embeddings to send\n",
    "# embeddings from strategy 1 or strategy 2 or custom strategy..\n",
    "subset_domain_convs, subset_sents, subset_embs = select_subset_serivces(subset_services, embs_hist1)\n",
    "\n",
    "print(f\"# Subset of domain {domain} conversations:\", len(subset_domain_convs))\n",
    "print(\"# Subset sentences:\", len(subset_sents))\n",
    "\n",
    "subset_embs = np.concatenate(subset_embs)\n",
    "print('Corresponding subset embeddings:', subset_embs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d391077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_transitions(domain_convs: List[list], cluster_ixs: np.ndarray) -> np.ndarray:    \n",
    "    \"\"\"Compute the transitions from one cluster to the other, based on\n",
    "    the actual conversation/dialogue flow. Also compute the assigment of sentence embeddings to \n",
    "    the cluster indices - mainly start of dialogue and end of dialogue sentences\"\"\"\n",
    "    \n",
    "    print('Computing transition matrix ..')\n",
    "    n_clusters = np.unique(cluster_ixs).size\n",
    "    print(' n_clusters', n_clusters)\n",
    "    \n",
    "    soc_clusters = []  # start of conversation clusters\n",
    "    eoc_clusters = []  # end of conversation clusters\n",
    "    # transitions: row(from) - col(to)\n",
    "    transitions = np.zeros(shape=(n_clusters, n_clusters))\n",
    "    \n",
    "    i = 0    \n",
    "    k = 0\n",
    "    while i < len(domain_convs): # iterate over conversations/dialogs, where each conv is a seq of utts\n",
    "        j = 0\n",
    "        prev_cix = -1\n",
    "        while j < len(domain_convs[i]):  # iterate over utts in the current dialogue\n",
    "        \n",
    "            cix = cluster_ixs[k]\n",
    "            \n",
    "            if j == 0:  # start of conversation\n",
    "                soc_clusters.append(cix)\n",
    "            else:\n",
    "                # from, to\n",
    "                transitions[prev_cix, cix] += 1                               \n",
    "            \n",
    "            prev_cix = deepcopy(cix)\n",
    "            \n",
    "            k += 1\n",
    "            j += 1\n",
    "            \n",
    "        # end of conversation\n",
    "        eoc_clusters.append(prev_cix)\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "    return transitions, np.asarray(soc_clusters), np.asarray(eoc_clusters)\n",
    "\n",
    "\n",
    "def get_cluster_assignments(occurrences):\n",
    "    \"\"\"Get percentage of embeddings assigned to each cluster\"\"\"\n",
    "    \n",
    "    ixs, count = np.unique(occurrences, return_counts=True)\n",
    "    max_ix = np.argmax(count)\n",
    "    print(\n",
    "        \"Cluster ix:\",\n",
    "        np.array2string(ixs, precision=0, formatter={'int_kind': lambda x: \"%4d\" % x}, separator=' |')\n",
    "    )\n",
    "    print(\n",
    "        \"Percentage:\",\n",
    "        np.array2string(count * 100/count.sum(), precision=1, \n",
    "                        formatter={'float_kind': lambda x: \"%4.1f\" % x},\n",
    "                        separator=' |')\n",
    "    )\n",
    "    print(\"{:.1f} % from cluster index {:d} is the most dense.\".format(count[max_ix]*100./count.sum(), ixs[max_ix]))\n",
    "    return count/count.sum(), ixs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db975d21",
   "metadata": {},
   "source": [
    "#### Clustering with k-means\n",
    "- Here we will cluster the subst of the embeddings using simple k-means clustering.\n",
    "- You may also try other clustering strategies based on Gaussian Mixture Models or even neural network based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268116ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you may play with this number, but it is better to keep it between [avg_utts_per_conv/2, avg_utts_per_conv x 2]\n",
    "print(\"Avg. utts per conversation: {:.0f}\".format(avg_utts_per_conv))\n",
    "\n",
    "\n",
    "def run_kmeans(n_clusters, embs, seed=12345):\n",
    "    \"\"\"Cluster the given embeddings into the specified number of clusters\n",
    "    and return the hard cluster assignments\"\"\"\n",
    "\n",
    "    print(\"Clustering\", embs.shape[0], \"embeddings into\", n_clusters, \"clusters with k-means..\")\n",
    "\n",
    "    norm_embs = preprocessing.normalize(embs)\n",
    "    print(\"  Normalizing embeddings to have unit length..\")\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=seed, max_iter=1000)\n",
    "    pred_ixs = kmeans.fit_predict(norm_embs)\n",
    "    print('.. done')\n",
    "    \n",
    "    return pred_ixs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3514d18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ixs = run_kmeans(12, embs_indep)  # kmeans with embeddings from strategy 1\n",
    "# pred_ixs = run_kmeans(12, embs_hist1)  # kmeans with embeddings from strategy 2\n",
    "# pred_ixs = run_kmeans(12, subset_embs)  # kmeans with subset of embeddings strategy 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008d794c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the embeddings are more or less equally distributed\n",
    "\n",
    "_ = get_cluster_assignments(pred_ixs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa318e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans, soc, eoc = compute_transitions(domain_convs, pred_ixs)  # strategy 1\n",
    "# trans, soc, eoc = compute_transitions(domain_convs, pred_ixs)  # strategy 2\n",
    "# trans, soc, eoc = compute_transitions(subset_domain_convs, pred_ixs)  # strategy 2 with subset\n",
    "print(\"Transition matrix:\", trans.shape)\n",
    "for i in range(trans.shape[0]):\n",
    "    trans[i, :] /= trans[i, :].sum()\n",
    "    \n",
    "print(\"\\nStart of conversation..\")\n",
    "# get the clusters where the conversation usually BEGIN\n",
    "soc_prob, soc_ixs = get_cluster_assignments(soc)\n",
    "\n",
    "\n",
    "print(\"\\nEnd of conversation..\")\n",
    "# get the clusters where the conversation usually END\n",
    "eoc_prob, eoc_ixs = get_cluster_assignments(eoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f758bdea",
   "metadata": {},
   "source": [
    "### Let us visualize the clusters and the transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0920df3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let us visualize the clusters and the transitions\n",
    "# We can set thresholds on the transition scores which will enable to control\n",
    "# the arcs that will be drawn\n",
    "\n",
    "def visualize_graph(trans, soc_prob, eoc_prob):\n",
    "\n",
    "    # see https://graphviz.readthedocs.io/en/stable/  for more documentation on graphviz\n",
    "\n",
    "    dot = graphviz.Digraph(f\"MutliWoz_{domain}\", format='png', graph_attr={'rankdir':'LR'})  # initialize a graph\n",
    "    \n",
    "    n_clusters = trans.shape[0]\n",
    "\n",
    "    # add BEGIN and END nodes\n",
    "    dot.node('BEGIN', shape='doublecircle')\n",
    "    dot.node('END', shape='doublecircle')\n",
    "\n",
    "    # Add a node representing each cluster\n",
    "    for i in range(n_clusters):\n",
    "        dot.node(str(i), shape='circle')    \n",
    "\n",
    "    # low threshold will show many arcs, higher threshold will only show dominant arcs\n",
    "    s_thresh = 0.1  # threshold for drawing begin, end transitions\n",
    "    thresh = 0.12  # threshold for drawing an intermediate arc\n",
    "\n",
    "    # draw arrows from BEGIN to that cluster(s) where the start sentences live\n",
    "    # given that they are above the `s_thresh`\n",
    "    for i, prob in enumerate(soc_prob):\n",
    "        if prob > s_thresh:\n",
    "            dot.edge('BEGIN', str(soc_ixs[i]), label=\"{:.2f}\".format(prob))\n",
    "            dot.node(str(soc_ixs[i]), fillcolor='cyan', style='filled')\n",
    "\n",
    "    # draw arrows from cluster(s) where the end sentences live to the END node\n",
    "    for i, prob in enumerate(eoc_prob):\n",
    "        if prob > s_thresh:\n",
    "            dot.edge(str(eoc_ixs[i]), 'END', label=\"{:.2f}\".format(prob))\n",
    "            dot.node(str(eoc_ixs[i]), fillcolor='pink', style='filled')\n",
    "\n",
    "    # draw intermediate arcs among the clusters where transitions > thresh\n",
    "\n",
    "    for row_ix in range(trans.shape[0]):  # row_ix represent FROM, col_ixs represent TO\n",
    "        row_trans = trans[row_ix, :] / trans[row_ix, :].sum()    \n",
    "        thresh_ixs = np.where(row_trans > thresh)\n",
    "        if thresh_ixs[0].size > 0:\n",
    "            # print(row_ix, '->', thresh_ixs)\n",
    "            for col_ix in thresh_ixs[0]:\n",
    "                dot.edge(str(row_ix), str(col_ix), label=\"{:.2f}\".format(row_trans[col_ix]))\n",
    "\n",
    "    # show the image within the notebook\n",
    "    # dot.view() will open the image in an external window\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53b40c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot = visualize_graph(trans, soc_prob, eoc_prob)\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07a616b",
   "metadata": {},
   "source": [
    "### What does each cluster (node) actually represent :\n",
    "* Can we get any representative utterances from the dialogues ?\n",
    "* Can we get any representative words (word cloud) for each cluster ?\n",
    "* Can we automatically identify the underlying intents / slot values for each cluster ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452dcc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_representative_words(sentences: list, pred_ixs: np.ndarray):\n",
    "    \"\"\"Get most representative words per each cluster, based on \n",
    "    relative counts normalized to yield probabilities\n",
    "    \"\"\"\n",
    "    n_clusters = np.max(pred_ixs) + 1\n",
    "\n",
    "    count_vect = CountVectorizer(stop_words='english', lowercase=True, strip_accents=None, min_df=2)\n",
    "    counts = count_vect.fit_transform(sentences)\n",
    "\n",
    "    print('counts (n_sentences x vocab_size):', counts.shape)\n",
    "    int2vocab = {}\n",
    "    for vocab, idx in count_vect.vocabulary_.items():\n",
    "        int2vocab[idx] = vocab\n",
    "\n",
    "    # cluster assignments to sentences in sparse format\n",
    "    # shape = (n_clusters x n_sentences)\n",
    "    sparse_cixs = sparse.csr_matrix((np.ones(len(pred_ixs,)), (pred_ixs, np.arange(len(pred_ixs)))))\n",
    "\n",
    "    # cumulative word count per cluster\n",
    "    wc_per_cluster = sparse_cixs.dot(counts)\n",
    "    print('n_clusters x vocab_size:', wc_per_cluster.shape, type(wc_per_cluster))\n",
    "\n",
    "    prob_per_cluster = wc_per_cluster / wc_per_cluster.sum(axis=0)\n",
    "\n",
    "    # print(prob_per_cluster.shape)\n",
    "    print()\n",
    "    \n",
    "    top_k = 15  # print TOP K words per cluster\n",
    "    \n",
    "    for i in range(n_clusters):\n",
    "        sort_ixs = np.argsort(prob_per_cluster[i, :].A.squeeze())[::-1]\n",
    "        print(\"= Cluster\", i, end=\"=\\n \")\n",
    "        for j in range(top_k):\n",
    "            print(int2vocab[sort_ixs[j]], end=\", \")\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6401bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_most_representative_words(sentences_indep, pred_ixs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b09b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(num=1)\n",
    "\n",
    "n_clusters = np.max(pred_ixs) + 1\n",
    "\n",
    "# https://matplotlib.org/stable/tutorials/colors/colormaps.html\n",
    "im = plt.imshow(trans, cmap='Purples')\n",
    "\n",
    "plt.xticks(np.arange(n_clusters))\n",
    "plt.yticks(np.arange(n_clusters))\n",
    "\n",
    "plt.ylabel(\"\\\"From\\\" cluster\")\n",
    "plt.xlabel(\"\\\"To\\\" cluster\")\n",
    "\n",
    "plt.title(\"Transitions among clusters\")\n",
    "\n",
    "_ = plt.colorbar(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2244fbc",
   "metadata": {},
   "source": [
    "- At this point, you can stop and go back and try \n",
    "1. different strategies to represent sentences\n",
    "2. and/or use different model to extract embeddings\n",
    "3. and/or use different number of clusters or clustering techniques such as Agglomerative Hierarchical Clustering, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e98531",
   "metadata": {},
   "source": [
    "# Advanced:\n",
    "- Below, we can train a model to represent our data, i.e., sentences from one of the strategies.\n",
    "- Then, we can extract most representative words per cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7950a388",
   "metadata": {},
   "source": [
    "### The following steps describe the generative process of our model \n",
    "\n",
    "1. Given a vocabulary of size $W$, and corresponding $k$ dimensional-word embeddings or a $k$-dimensional subspace defined by\n",
    "\n",
    "    $\\mathbf{E} \\in \\mathbb{R}^{W \\times k}$\n",
    "\n",
    "2. For each document $d=1 \\ldots D$ \n",
    "\n",
    "    Sample a document-specific embedding as follows:\n",
    "\n",
    "    $\\mathbf{a}_d \\sim \\mathcal{N}(\\mathbf{a}_d \\mid \\mathbf{0}, \\mathbf{I})$\n",
    "    \n",
    "    Obtain the distribution of words in each document $\\boldsymbol{\\theta}_d$ by normalizing\n",
    "\n",
    "    $\\boldsymbol{\\theta}_d = \\mathrm{softmax}(\\mathbf{b} + \\mathbf{E} \\, \\mathbf{a}_d)$\n",
    "    \n",
    "    where $\\mathbf{b}$ is a $V$ dimensional vector representing bias.    \n",
    "\n",
    "    Generate a vector of words of a document (draw $N_d$ tokens independently) by sampling\n",
    "\n",
    "    $\\mathbf{x}_d \\sim \\mathrm{Multinomial}(\\boldsymbol{\\theta}_d, N_d) $\n",
    "    \n",
    "$\\mathbf{x}_d$ for $d=1 \\ldots D$ put row-wise in matrix $\\mathbf{X}$ represents the document-by-word statistics of the dataset.\n",
    "\n",
    "### Reality\n",
    "\n",
    "In reality, we do not generate the data, instead, given the document-by-word counts $\\mathbf{X}$ (rows represent document indices, and columns represent word indices), we would like to estimate the word embedding matrix $\\mathbf{E}$, bias vector $\\mathbf{b}$ and document embeddings $\\mathbf{a}_d, \\forall \\, d=1 \\ldots D$, that best explains (maxmizes the likelihood) the given (observed) counts $\\mathbf{X}$.\n",
    "\n",
    "### Likelihood\n",
    "\n",
    "Since we assumed each document $d$ is sampled from a $\\mathrm{Multonomial}$ distribution with parameters $\\boldsymbol{\\theta}_d$, we can compute the likelihood of a document as\n",
    "\n",
    "$p(\\mathbf{x}_d \\mid \\boldsymbol{\\theta}_d) = \\prod_{i=1}^{W} \\theta_{di} ^ {x_{di}}$\n",
    "\n",
    "or the log of the likelihood is\n",
    "\n",
    "$\n",
    "\\log p(\\mathbf{x}_d \\mid \\boldsymbol{\\theta}_d) = \\sum_{i} x_{di} \\log (\\theta_{di}) \\\\\n",
    "\\qquad \\qquad \\quad = \\sum_{i} x_{di} \\log \\Big( \\frac{\\exp\\{b_i + \\mathbf{e}_i^{T} \\mathbf{a}_d\\}}{\\sum_{j=1}^{W}\\exp\\{b_j + \\mathbf{e}_j^{T} \\mathbf{a}_d\\}} \\Big) \\\\\n",
    "\\qquad \\qquad \\quad = \\sum_{i} x_{di} \\Big[(b_i + \\mathbf{e}_i^{T} \\mathbf{a}_d) - \\log \\big(\\sum_{j}\\exp\\{b_j + \\mathbf{e}_j^{T} \\mathbf{a}_d\\} \\big) \\Big]\n",
    "$\n",
    "\n",
    "\n",
    "Log-likelihood for all the documents is the summation over individual log-likelihoods\n",
    "\n",
    "$\\mathcal{L} = \\sum_{d=1}^{D} \\log p(\\mathbf{x}_d \\mid \\boldsymbol{\\theta}_d) \\\\\n",
    "\\,\\,\\,\\, = \\sum_{d=1}^{D} \\sum_{i} x_{di} \\Big[(b_i + \\mathbf{e}_i^{T} \\mathbf{a}_d) - \\log \\big(\\sum_{j}\\exp\\{b_j + \\mathbf{e}_j^{T} \\mathbf{a}_d\\} \\big) \\Big]\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8a299b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import log_softmax, logsumexp\n",
    "\n",
    "class Model:\n",
    "    \"\"\"Model defintion, parameters and helper fucntions to compute log-likelihood\"\"\"\n",
    "    \n",
    "    def __init__(self, D, W, K, doc_embs):\n",
    "        \"\"\"Initialize our model\n",
    "        \n",
    "        Args:\n",
    "            D: number of documents\n",
    "            W: vocab size / number of words\n",
    "            K: embedding dimension / subspace\n",
    "        \"\"\"\n",
    "        \n",
    "        self.D = D\n",
    "        self.W = W\n",
    "        self.K = K\n",
    "        \n",
    "        # word embeddings matrix / subspace \n",
    "        n1 = 1. / np.sqrt(K)\n",
    "        n2 = 1. / np.sqrt(W)\n",
    "        self.E = np.random.uniform(-n2, n1, size=(W, K))\n",
    "        \n",
    "        # bias vector\n",
    "        self.b = np.random.randn(W, 1)  * 0.001\n",
    "        \n",
    "        # document embeddings\n",
    "        # n3 = 1. / np.sqrt(D)\n",
    "        # self.A = np.random.uniform(-n1, n3, size=(K, D))\n",
    "        if doc_embs.shape[1] == K:\n",
    "            doc_embs = doc_embs.T\n",
    "        self.A = doc_embs\n",
    "        \n",
    "        print('  Model params')\n",
    "        print('    Word embs:', self.E.shape)\n",
    "        print('    Doc  embs:', self.A.shape)\n",
    "        \n",
    "    def init_bias_with_log_unigram_dist(self, X):\n",
    "        \"\"\"We will initialize the bias vector with log of unigram distribution over vocabulary.\n",
    "        This should help us with better initialization.\n",
    "        \n",
    "        b = \\log (\\sum_d x_d) / (\\sum_d \\sum_i x_{di})\n",
    "        \"\"\"\n",
    "        \n",
    "        # if X is sparse matrix, X.A gives the dense version of it in numpy array format\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = X + 1e-08  # to avoid zeros\n",
    "        else:\n",
    "            X = X.A + 1e-08  # to avoid any zeros\n",
    "        \n",
    "        self.b[:, 0] = np.log(X.sum(axis=0) / X.sum())  # we would like b to of size (W, 1)\n",
    "        \n",
    "    def compute_log_thetas(self, sanity_check=False):\n",
    "        \"\"\"Compute log of thetas, where theta_d is the unigram distribution over document `d` estiamted from\n",
    "        the current params (word-embedding matrix, bias vector) and document embedding a_d. \"\"\"\n",
    "        \n",
    "        mat = self.b + (self.E @ self.A)  # shape is W x D\n",
    "        mat = mat.T  # shape is D x W\n",
    "        \n",
    "        # log_norm = logsumexp(mat, axis=1)\n",
    "        # log_thetas = mat - log_norm\n",
    "        \n",
    "        # the following single step is same the two above steps combined\n",
    "        log_thetas = log_softmax(mat, axis=1)  # shape is D x W\n",
    "        \n",
    "        if sanity_check:\n",
    "            # sanity-check\n",
    "            # since each document is a proper distribution, it should sum upto 1\n",
    "            # sum of the matrix should be equal to number of documents\n",
    "            print(\"Sanity check for log-thetas:\",\n",
    "                np.allclose(np.exp(log_thetas).sum(), self.D)\n",
    "            )\n",
    "\n",
    "        return log_thetas\n",
    "        \n",
    "    def compute_log_likelihood(self, X):\n",
    "        \"\"\"Compute log-likelihood of the data, given the current parameters / embeddings\n",
    "        \n",
    "        Each summation could be implemented using a for-loop but that would very slow, \n",
    "        since we have every thing stored in matrices and a sparse matrix, we will do it via\n",
    "        matrix muliplications and additions.\n",
    "        \n",
    "        Args:\n",
    "            X: doc-by-word counts in scipy.sparse format\n",
    "            \n",
    "        Returns:\n",
    "            float: log-likelihood of the data\n",
    "        \"\"\"\n",
    "        \n",
    "        log_thetas = self.compute_log_thetas()\n",
    "        \n",
    "        # log-likelihood is product of counts to the respective log-probability values.\n",
    "        if isinstance(X, np.ndarray):\n",
    "            llh = (X * log_thetas).sum()            \n",
    "        else:\n",
    "            # X is a scipy sparse matrix\n",
    "            llh = (X.multiply(log_thetas)).sum()\n",
    "        \n",
    "        return llh\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e18cf0e",
   "metadata": {},
   "source": [
    "### Training the model / updating the parameters\n",
    "\n",
    "We will keep the bias vector fixed to initial value, which is log of unigram distribution over the vocabulary.\n",
    "\n",
    "We would like to obtain the parameters that maximize the log-likelihood. We follow gradient-ascent to achieve this target.\n",
    "\n",
    "We will make these updates alternatively, i.e.,\n",
    "\n",
    "1. In one-step, we will fix all the document-embeddings $\\mathbf{a}_d \\, \\forall \\, d$, and update the word-embedding matrix $\\mathbf{E}$.\n",
    "\n",
    "2. In the next-step, we will update document-embeddings $\\mathbf{a}_d \\, \\forall \\, d$, by keeping word-embeddings fixed.\n",
    "\n",
    "\n",
    "### Obtaining gradients of the objective (log-likelihood) with respect to word embeddings\n",
    "\n",
    "In the following, we derive the gradient of the log-likelihood $\\mathcal{L}$ with respect to the word embeddings, i.e., **each row** $\\mathbf{e}_k$ in $\\mathbf{E}$.\n",
    "\n",
    "**NOTE:** $\\mathbf{e}_k$ is a row-vector, hence the final gradient will also be a row-vector.\n",
    "\n",
    "Let us re-write the log-likelihood once again\n",
    "\n",
    "$\\mathcal{L} = \\sum_{d=1}^{D} \\sum_{i} x_{di} \\Big[(b_i + \\mathbf{e}_i^{T} \\mathbf{a}_d) - \\log \\big(\\sum_{j}\\exp\\{b_j + \\mathbf{e}_j^{T} \\mathbf{a}_d\\} \\big) \\Big]\n",
    "$\n",
    "\n",
    "$\n",
    "\\nabla_{e_k} \\mathcal{L} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{e}_k} =\\frac{\\partial  \\sum_{d=1}^{D} \\sum_{i} x_{di} \\Big[(b_i + \\mathbf{e}_i^{T} \\mathbf{a}_d) - \\log \\big(\\sum_{j}\\exp\\{b_j + \\mathbf{e}_j^{T} \\mathbf{a}_d\\} \\big) \\Big]}{\\partial \\mathbf{e}_k} \\\\\n",
    "\\qquad \\qquad = \\sum_d \\sum_i x_{di} \\Big[ \\frac{\\partial (b_i + \\mathbf{e}_i^{T} \\mathbf{a}_d)}{\\partial \\mathbf{e}_k}   \\Big] - \\sum_i x_{di} \\Big[ \\frac{\\partial \\log \\big(\\sum_{j}\\exp\\{b_j + \\mathbf{e}_j^{T} \\mathbf{a}_d\\} \\big)}{\\partial \\mathbf{e}_k} \\Big] \\\\\n",
    "\\qquad \\qquad = \\sum_d \\Big[ x_{dk}(\\mathbf{a}_d^T) \\Big] - \\sum_i x_{di} \\Big[ \\frac{1}{\\sum_j \\exp \\{b_j + \\mathbf{e}_j^T \\mathbf{a}_d \\}} \\big( \\frac{\\partial \\sum_j \\exp\\{b_j + \\mathbf{e}_j^T \\mathbf{a}_d\\} }{\\partial \\mathbf{e}_k} \\big) \\Big] \\\\\n",
    "\\qquad \\qquad = \\sum_d \\Big[ x_{dk}\\mathbf{a}_d^T \\Big] - \\sum_i x_{di} \\Big[ \\frac{1}{\\sum_j \\exp \\{b_j + \\mathbf{e}_j^T \\mathbf{a}_d \\}} \\big( 0 + \\ldots + \\exp \\{b_k + \\mathbf{e}_k^T \\mathbf{a}_d \\} \\mathbf{a}_d^T + 0 + \\ldots \\big) \\Big] \\\\\n",
    "\\qquad \\qquad = \\sum_d \\Big[ x_{dk}\\mathbf{a}_d^T \\Big] - \\sum_i x_{di} \\Big[\\frac{ \\exp \\{b_i + \\mathbf{e}_k^T  \\mathbf{a}_d^T\\}}{\\sum_j \\exp \\{b_j + \\mathbf{e}_j^T \\mathbf{a}_d \\}}\\mathbf{a}_d^T  \\Big] \\\\\n",
    "\\qquad \\qquad = \\sum_d \\Big[ x_{dk}\\mathbf{a}_d^T \\Big] - \\sum_i x_{di} \\Big[ \\theta_{dk} \\mathbf{a}_d^T \\Big] \\\\\n",
    "\\qquad \\qquad = \\sum_d \\Big[x_{dk} - (\\sum_{i} x_{di}) \\theta_{dk} \\Big] \\mathbf{a}_d^T\n",
    "$\n",
    "\n",
    "Interpretation of the gradient (i.e., derivate of log-likelihood $\\mathcal{L}$ w.r.t a word embedding for word $k$)\n",
    "\n",
    "$x_{dk}$: number of times word $k$ appeared in document $d$ \n",
    "\n",
    "$\\theta_{dk}$: the estimated probability of word $k$ in document $d$\n",
    "\n",
    "$\\sum_i x_{di}$: sum of all the word counts in document $d$\n",
    "\n",
    "\n",
    "$(\\sum_i x_{di}) \\theta_{dk}$: weigh the `sum of all word counts` in document $d$ by the probability of word $k$ in $d$, which kind of gives the relative word count of word $k$ in document $d$\n",
    "\n",
    "$\\Big[x_{dk} - (\\sum_{i} x_{di}) \\theta_{dk} \\Big] \\mathbf{a}_d^T$: the difference of absoulte word count to the relative word count, and weight this along the direction of document embedding. In the beginning of training this difference is not zero as $\\theta_{dk}$ is obatained from random initilization. Towards the end of training, the estimated $\\theta_{dk}$ should be close the maximum likelihood $\\widehat{\\theta}_{dk} = \\frac{x_{dk}}{\\sum_{i} x_{di}}$\n",
    "\n",
    "The final gradient is the sum of all weighted document embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa1bea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradients_E(model, X):\n",
    "    \"\"\"Gradient of the log-likelihood with-respect-to word embedding matrix `E`\n",
    "    \n",
    "    Args:\n",
    "        model (Model): The object of the model \n",
    "        X (scipy.sparse_matrix): The doc-by-word counts\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Gradient of log-likelihood w.r.t word embeddings, i.e, grad of llh w.r.t to model.E\n",
    "    \"\"\"\n",
    "    \n",
    "    # grads = np.zeros_like(model.E)  # initialize empty gradients to be the same shape as word embeddings (W, K)\n",
    "    \n",
    "    # compute log_thetas as they are needed in gradient\n",
    "    log_thetas = model.compute_log_thetas()\n",
    "    \n",
    "    # the gradient computation can be done using for-loops to reflect the equation\n",
    "    # or it can be done efficiently using matrix multiplications\n",
    "    \n",
    "    # 1. simple way using for-loop    \n",
    "    # iterate over all documents\n",
    "    # for d in range(model.D):\n",
    "        \n",
    "        # iterate over every word, \n",
    "    #     for k in range(model.W):\n",
    "    #         x_dk = X[d, k]  # count of word k in doc d\n",
    "    #         rel_x_dk = X[d, :].sum() * np.exp(log_thetas)[d, k]  # relative /estimated count of word k in doc d                  \n",
    "    #         grads[k, :] += ((x_dk - rel_x_dk) * model.A[:, d])  # doc embeddings are column wise in model.A\n",
    "            \n",
    "    # 2. Efficient way of obtaining gradients using matrix operations\n",
    "    \n",
    "    ef_grads = np.zeros_like(model.E)\n",
    "    \n",
    "    tmp = (X - np.multiply(X.sum(axis=1).reshape(-1, 1), np.exp(log_thetas))).A  # .A will convert matrix to np ndarray\n",
    "    ef_grads = (model.A @ tmp).T\n",
    "    \n",
    "    # Sanity check to see if gradients computed in both ways are numerically identical\n",
    "    # print('- All close grad_E:', np.allclose(ef_grads, grads))\n",
    "    \n",
    "    return ef_grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e824f066",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradients_A(model, X):\n",
    "    \"\"\"Gradient of the log-likelihood with-respect-to document embeddings.\n",
    "    WE WILL NOT USE IT BECAUSE, WE ALREADY HAVE DOCUMENT/SENTENCE EMBEDDINGS\n",
    "    FROM PRETRAINED MODEL\n",
    "    \n",
    "     Args:\n",
    "        model (Model): The object of the model \n",
    "        X (scipy.sparse_matrix): The doc-by-word counts\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Gradient of log-likelihood w.r.t document embeddings, i.e, grad of llh w.r.t to model.A\n",
    "    \"\"\"\n",
    "    \n",
    "    grads = np.zeros_like(model.A)  # initialize empty gradients to be the same shape as doc embeddings (K, D)\n",
    "    \n",
    "    # compute log_thetas as they are needed in gradient\n",
    "    log_thetas = model.compute_log_thetas()\n",
    "    \n",
    "    \n",
    "    ##\n",
    "    ## -- IMPLEMENT THE GRADIENT COMPUTATION HERE (TASK 2 of 4)\n",
    "    ## The final gradients should be obtained using only matrix multiplications - no for loops\n",
    "    \n",
    "    tmp = (X - np.multiply(X.sum(axis=1).reshape(-1, 1), np.exp(log_thetas))).A  # .A will convert matrix to np ndarray\n",
    "    grads = (tmp @ model.E).T\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c987b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(params, gradient, learning_rate):\n",
    "    \"\"\"Update the parameters\n",
    "    \n",
    "    Args:\n",
    "        params (np.ndarray): Word embedding matrix of the document embedding matrix\n",
    "        gradient (np.ndarray): Gradients of all word embeddings or document embeddings. Should be same as size as params\n",
    "        learning_rate (float): The learning_rate can also be seen as step size, i.e, the size of the step to be taken\n",
    "               along the direction of gradient. Too big steps can overshoot our estimate, whereas too small steps\n",
    "               can take longer for the model to reach optimum.\n",
    "               \n",
    "    Returns:\n",
    "        np.ndarray: the updated params\n",
    "    \"\"\"\n",
    "    \n",
    "    assert params.shape == gradient.shape, \"The params and gradient must have same shape, \\\n",
    "    ({:d}, {:d}) != ({:d} {:d})\".format(*params.shape, *gradient.shape)\n",
    "    \n",
    "    new_params = params + (learning_rate * gradient)  # since we are doing gradient ascent\n",
    "    return new_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0aea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X, train_iters, learning_rate):\n",
    "    \"\"\"Training scheme for the model\"\"\"\n",
    "    \n",
    "    llh_0 = model.compute_log_likelihood(X)\n",
    "    print(\"  Initial log-likelihood: {:16.2f}\".format(llh_0))\n",
    "    \n",
    "    llhs = [llh_0]\n",
    "    \n",
    "    # pbar = tqdm(np.arange(1, train_iters+1).tolist())\n",
    "    \n",
    "    tran = tqdm.trange(train_iters, desc='Log-likelihood {:10.1f} | Training progress'.format(llh_0), leave=True)\n",
    "    \n",
    "    for i in tran:       \n",
    "        \n",
    "        # First-step: update word embeddings E, by keeping doc-embeddings A fixed\n",
    "        \n",
    "        grad_E = gradients_E(model, X)\n",
    "\n",
    "        model.E = update_parameters(model.E, grad_E, learning_rate['E'])\n",
    "        \n",
    "        llh_ei = model.compute_log_likelihood(X)\n",
    "        # print(\" Update E log-likelihood: {:16.2f}\".format(llh_ei))\n",
    "        # pbar.set_description(\"Log-likelihood: {:16.2f}\".format(llh_ei))\n",
    "        tran.set_description(\"Log-likelihood {:10.1f} | Training progress\".format(llh_ei))\n",
    "        tran.refresh() # to show immediately the update\n",
    "        \n",
    "        if llh_ei < llhs[-1]:\n",
    "            print(\"The log-likelihood should improve. Instead it decreased, which means the updates have overshooted, \\\n",
    "decrease the learning_rate (step_size). Additionally, you could make learning_rate as a function of \\\n",
    "iteration so that it decays slowly. Or you could implement back-tracking by halving the learning rate. But this is not \\\n",
    "required for your home-work.\")\n",
    "            break            \n",
    "        \n",
    "        llhs.append(llh_ei)\n",
    "        \n",
    "        # Next-step (alternating): # First-step: update doc embeddings A, by keeping word-embeddings E fixed\n",
    "        \n",
    "        # grad_A = gradients_A(model, X)     \n",
    "        \n",
    "        # model.A = update_parameters(model.A, grad_A, learning_rate[\"A\"])\n",
    "        \n",
    "        # llh_ai = model.compute_log_likelihood(X) \n",
    "        # print(\" Update A log-likelihood: {:16.2f}\".format(llh_ai))\n",
    "        \n",
    "        # if llh_ai < llhs[-1]:\n",
    "        #    print(\"The log-likelihood should improve. Instead it decreased, which means the updates have overshooted, \\\n",
    "#decrease the learning_rate (step_size). Additionally, you could make learning_rate as a function of \\\n",
    "#iteration so that it decays slowly. Or you could implement back-tracking by halving the learning rate. But this is not \\\n",
    "#required for your home-work.\")\n",
    "#            break            \n",
    "        \n",
    "        # llhs.append(llh_ai)\n",
    "        \n",
    "        # learning_rate scheduler\n",
    "        # we reduce the learning_rate by 10% after every iteration\n",
    "        learning_rate['E'] -= (learning_rate['E'] * 0.1)\n",
    "        # learning_rate['A'] -= (learning_rate['A'] * 0.1)\n",
    "        \n",
    "    return model, llhs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f32a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will used only a subset of sentences from strategy 2\n",
    "\n",
    "count_vect = CountVectorizer(stop_words=None, lowercase=False, strip_accents=None, min_df=1)\n",
    "DbyW = count_vect.fit_transform(subset_sents)\n",
    "print('Counts (DbyW)', DbyW.shape)\n",
    "\n",
    "int2vocab = {}\n",
    "for word, idx in count_vect.vocabulary_.items():\n",
    "    int2vocab[idx] = word\n",
    "\n",
    "np.random.seed(99)\n",
    "\n",
    "# higher the embedding-dim, more semantic information could be captured, but at the same time\n",
    "# increases the number of parameters and also training time and memory requriements.\n",
    "\n",
    "# you could play with emb_dim = {20, 50, 64, 80, 100, 128}\n",
    "\n",
    "emb_dim = subset_embs.shape[1]  # embedding dim\n",
    "print('Embedding dim of sentences:', emb_dim)\n",
    "model = Model(DbyW.shape[0], DbyW.shape[1], emb_dim, subset_embs)\n",
    "\n",
    "initial_llh = model.compute_log_likelihood(DbyW)\n",
    "print(\"Initial  log-likelihood:\", initial_llh)\n",
    "\n",
    "print(\"Initializing bias vector to log-unigram distribution.\")\n",
    "model.init_bias_with_log_unigram_dist(DbyW)\n",
    "\n",
    "new_init_llh = model.compute_log_likelihood(DbyW)\n",
    "print(\"New init log-likelihood:\", new_init_llh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984961fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iters = 50\n",
    "\n",
    "# we could use different learning_rate for word and document embeddings\n",
    "# smaller learning rates can ensure that updates do not overshoot, but at the same time, it can many iterations\n",
    "# for the model to converge\n",
    "\n",
    "# higher learning rate can make big steps, but can overshoot, one can use backtracking approach to make smaller steps.\n",
    "\n",
    "# You can play with various learning rate and see how quickly the model converges\n",
    "learning_rate = {'E': 0.0005, 'A': 0.001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02af3bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train the model\n",
    "# Run this cell again to continue traning for more iterations\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "os.environ['MKL_NUM_THREADS'] = '4'\n",
    "model, llhs = train(model, DbyW, train_iters, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d02cd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_log_likelihood(llhs, fig_num):\n",
    "        \n",
    "    plt.figure(fig_num)\n",
    "    plt.plot(np.arange(0, len(llhs)/2., 0.5), llhs, '.-')\n",
    "    \n",
    "    plt.xlabel('Training iterations')\n",
    "    plt.ylabel('Log-likelihood')\n",
    "    \n",
    "    plt.grid(alpha=0.5, linestyle='--')\n",
    "    plt.show()\n",
    "    \n",
    "plot_log_likelihood(llhs, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6caf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster and Get more representative words using the trained word embeddings\n",
    "\n",
    "def cluster_and_get_representative_words(int2vocab, model, n_clusters):\n",
    "\n",
    "    embs = model.A.T\n",
    "    print('Doc embeddings (model.A.T):', model.A.T.shape)\n",
    "\n",
    "    print(\"Clustering\", embs.shape[0], \"embeddings into\", n_clusters, \"clusters with k-means..\")\n",
    "\n",
    "    norm_embs = preprocessing.normalize(embs)\n",
    "    print(\"  Normalizing embeddings to have unit length..\")\n",
    "\n",
    "    kmeans2 = KMeans(n_clusters=n_clusters, random_state=12345, max_iter=1000)\n",
    "    pred_ixs2 = kmeans2.fit_predict(norm_embs)\n",
    "    print('.. done')\n",
    "    \n",
    "    cluster_ixs, cluster_strength = np.unique(pred_ixs2, return_counts=True)\n",
    "\n",
    "    cluster_strength = cluster_strength / pred_ixs2.shape[0]\n",
    "\n",
    "    centroids = kmeans2.cluster_centers_\n",
    "\n",
    "    print(\"Cluster centers:\", centroids.shape)\n",
    "\n",
    "    global_mean = np.mean(embs, axis=0)\n",
    "\n",
    "    scores = (model.E @ (centroids - global_mean).T) \n",
    "    # scores = (model.E @ centroids.T) \n",
    "\n",
    "    topn = 15  # top N words per cluster\n",
    "\n",
    "    print(\"Cluster_index (strength in %) Representative tokens\\n\")\n",
    "    for k in range(n_clusters):\n",
    "        k_ixs = np.argsort(scores[:, k])[::-1] # sort in descending order\n",
    "        print(\"{:3d} ({:3.1f})\".format(k, cluster_strength[k]*100.), end=\"   \")\n",
    "        for i in range(topn):\n",
    "            print(int2vocab[k_ixs[i]], end=\", \")\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    print()\n",
    "    \n",
    "#     top_k = 15  # print TOP K words per cluster    \n",
    "#     for i in range(n_clusters):\n",
    "#         sort_ixs = np.argsort(prob_per_cluster[i, :].A.squeeze())[::-1]\n",
    "#         print(\"= Cluster\", i, end=\"=\\n \")\n",
    "#         for j in range(top_k):\n",
    "#             print(int2vocab[sort_ixs[j]], end=\", \")\n",
    "#         print(\"\\n\")\n",
    "        \n",
    "    return pred_ixs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a837d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ixs2 = cluster_and_get_representative_words(int2vocab, model, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43809c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans2, soc2, eoc2 = compute_transitions(subset_domain_convs, pred_ixs2)  # strategy 2 with subset\n",
    "print(\"Transition matrix:\", trans2.shape)\n",
    "for i in range(trans2.shape[0]):\n",
    "    trans2[i, :] /= trans2[i, :].sum()\n",
    "    \n",
    "print(\"\\nStart of conversation..\")\n",
    "# get the clusters where the conversation usually BEGIN\n",
    "soc_prob2, soc_ixs2 = get_cluster_assignments(soc2)\n",
    "\n",
    "\n",
    "print(\"\\nEnd of conversation..\")\n",
    "# get the clusters where the conversation usually END\n",
    "eoc_prob2, eoc_ixs2 = get_cluster_assignments(eoc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad9f91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot2 = visualize_graph(trans2, soc_prob2, eoc_prob2)\n",
    "dot2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144b31db",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_most_representative_words(subset_sents, pred_ixs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f773a2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
